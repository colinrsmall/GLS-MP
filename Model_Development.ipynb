{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Development",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinrsmall/GLS-MP/blob/master/Model_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7UvwwCM0wC7",
        "colab_type": "text"
      },
      "source": [
        "# GLS-MP Model Development\n",
        "\n",
        "**Authors:** Colin Small (crs1031@wildcats.unh.edu), Matthew Argall (Matthew.Argall@unh.edu), Marek Petrik (Marek.Petrik@unh.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdDrn6mBTUgT",
        "colab_type": "text"
      },
      "source": [
        "[![MMS Mission Video](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Artist_depiction_of_MMS_spacecraft_%28SVS12239%29.png/640px-Artist_depiction_of_MMS_spacecraft_%28SVS12239%29.png)](https://upload.wikimedia.org/wikipedia/commons/c/c9/NASA_Spacecraft_Finds_New_Magnetic_Process_in_Turbulent_Space.webm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL5p5pJQ39Dd",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "Global-scale energy flow throughout Earth’s magnetosphere is catalyzed by processes that occur at Earth’s magnetopause (MP) in the electron diffusion region (EDR) of magnetic reconnection. Until the launch of the Magnetospheric Multiscale (MMS) mission, only rare, fortuitous circumstances permitted a glimpse of the electron dynamics that break magnetic field lines and energize plasma. MMS employs automated burst triggers onboard the spacecraft and a  Scientist-in-the-Loop (SITL) on the ground to select intervals likely to contain diffusion regions. Only low-resolution survey data is available to the SITL, which is insufficient to resolve electron dynamics. A strategy for the SITL, then, is to select all MP crossings. This has resulted in over 35 potential MP EDR encounters but is labor- and resource-intensive; after manual reclassification, just ∼ 0.7% of MP crossings, or 0.0001% of the mission lifetime during MMS’s first two years contained an EDR.\n",
        "\n",
        "In this notebook, we develop a Long-Short Term Memory (LSTM) neural network to detect magnetopause crossings and automate the SITL classification process. An LSTM developed with this notebook has been implemented in the MMS data stream to provide automated predictions to the SITL.\n",
        "\n",
        "\n",
        "This model facilitates EDR studies and helps free-up mission operation costs by consolidating manual classification processes into automated routines.\n",
        "\n",
        "**Authors' note:** This notebook was developed after the development of the original model in use at the SDC. We have tried our best to replicate the development steps and hyperparamteres of that model, but we cannot guarantee that models developed with this notebook will exactly match the performance of the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Em6RZ8Z6UFl",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "To start, we import the neccesary libraries for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WshufyyiiLd",
        "colab_type": "code",
        "outputId": "ce59a2e4-432d-41c1-8c7b-875a1b37a288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "!pip install nasa-pymms"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nasa-pymms\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/f7/35f84cccf610ad4a35495f06748e4d24782d13a0f4f30dcae9096b6035bc/nasa_pymms-0.3.0-py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hCollecting cdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/d3/4c76dc94bc311ec779e9a2f662539028c7bd6ded1785bf27ead4e9f2f6a2/cdflib-0.3.18.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.6/dist-packages (from nasa-pymms) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from nasa-pymms) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from nasa-pymms) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from nasa-pymms) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nasa-pymms) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->nasa-pymms) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->nasa-pymms) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->nasa-pymms) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->nasa-pymms) (1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->nasa-pymms) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->nasa-pymms) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->nasa-pymms) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->nasa-pymms) (2.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.1->nasa-pymms) (1.12.0)\n",
            "Building wheels for collected packages: cdflib\n",
            "  Building wheel for cdflib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdflib: filename=cdflib-0.3.18-cp36-none-any.whl size=60627 sha256=f14b8ccb417c61e5776d802e870bb29ee84c4f342f4462635e50aadff7b30173\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/fa/1f/fcb5a80cdf3b529877d1a692fa246e5328986b24eaa22442c1\n",
            "Successfully built cdflib\n",
            "Installing collected packages: cdflib, nasa-pymms\n",
            "Successfully installed cdflib-0.3.18 nasa-pymms-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwnLUN2cUd4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b8f7cca1-f48a-487a-dc64-f1d6ed9ba16f"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from pathlib import Path\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization, Bidirectional, Reshape, TimeDistributed\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from keras import backend as K\n",
        "from pymms.sdc import mrmms_sdc_api as mms\n",
        "import keras.backend.tensorflow_backend as tfb\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import os\n",
        "import time\n",
        "import sklearn\n",
        "import scipy\n",
        "import pickle\n",
        "import random\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating root data directory /root/data/mms\n",
            "Creating root data directory /root/data/mms/dropbox\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-LAsDIG3XD7",
        "colab_type": "text"
      },
      "source": [
        "# Download, Preprocess, and Format MMS Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWIpfE2zSCOP",
        "colab_type": "text"
      },
      "source": [
        "After installing and importinng the neccesary libraries, we download our training and validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcytuTHUR8U8",
        "colab_type": "code",
        "outputId": "5024ee64-7874-4a5c-fd24-19196d904410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!wget -O training_data.csv https://zenodo.org/record/3884266/files/original_training_data.csv?download=1\n",
        "!wget -O validation_data.csv https://zenodo.org/record/3884266/files/original_validation_data.csv?download=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-12 22:33:45--  https://zenodo.org/record/3884266/files/original_training_data.csv?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 NOT FOUND\n",
            "2020-06-12 22:33:46 ERROR 404: NOT FOUND.\n",
            "\n",
            "--2020-06-12 22:33:48--  https://zenodo.org/record/3884266/files/original_validation_data.csv?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 NOT FOUND\n",
            "2020-06-12 22:33:49 ERROR 404: NOT FOUND.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDg-HJ0MAtjS",
        "colab_type": "text"
      },
      "source": [
        "After downloading the training and validation data, we preprocess our training data in preparation for training the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp4mcnLfFZXw",
        "colab_type": "text"
      },
      "source": [
        "We first load the data we downloaded above. The data is a table of measurements from the MMS spacecraft, where each row represents individual measurements taken at a given time and where each column represents a feature (variable) recorded at that time. There is an additional column representing the ground truths for each measurement (whether this measurement was selected by a SITL or not). Then, we will adjust the formatting and datatypes of several of the columns and sort the data by the time of the measurement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojTr8i472HY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "4707fba8-ae5b-4bb9-e963-58fb3e7e17e0"
      },
      "source": [
        "mms_data = pd.read_csv('training_data.csv', index_col=0, infer_datetime_format=True,\n",
        "\t\t\t\t\t\t   parse_dates=[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-da02f1970855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mms_data = pd.read_csv('training_data.csv', index_col=0, infer_datetime_format=True,\n\u001b[0;32m----> 2\u001b[0;31m \t\t\t\t\t\t   parse_dates=[0])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2yZv1MwGqWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mms_data[mms_data['selected'] == False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe4HZy7CHvFF",
        "colab_type": "text"
      },
      "source": [
        "We save references to data's index and column names for later use and additionally pop off the ground truths column. We will reattach the ground truths column after standardizing and interpolating the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2bm6URzIeFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = mms_data.index\n",
        "selections = mms_data.pop(\"selected\")\n",
        "column_names = mms_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gBjgkNsJvPV",
        "colab_type": "text"
      },
      "source": [
        "Since there exists a possibility that the training contains missing data or data misreported by the MMS spacecraft (reported as either infinity or negative infinity), we need to fill in (interpolate) any missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBg8Fds8KIhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mms_data = mms_data.replace([np.inf, -np.inf], np.nan)\n",
        "mms_data = mms_data.interpolate(method='time', limit_area='inside')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdMDvmcYLQ-1",
        "colab_type": "text"
      },
      "source": [
        "We normalize all features with standardization:\n",
        "\n",
        "![z = (x - u) / s ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b0aa2e7d203db1526c577192f2d9102b718eafd5)\n",
        "\n",
        "Where x̄ is the mean of the data, and σ is the standard deviation of the data.\n",
        "\n",
        "Normalization ensures that the numerical values of all features of the data fall within a range from one to negative one and are centered around their mean (zero-mean and unit variance). Normalization improves the speed and performance of training neural networks as it unifies the scale by which differences in the data are represented without altering the data themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXlE0dj_PbqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = preprocessing.StandardScaler()\n",
        "mms_data = scaler.fit_transform(mms_data)\n",
        "mms_data = pd.DataFrame(mms_data, index, column_names)\n",
        "mms_data = mms_data.join(selections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HytkkKWLR6KS",
        "colab_type": "text"
      },
      "source": [
        "Next, we calculate class weights for our data classes (selected data points and non-selected data points). Since the distribution of our data is heavily skewed towards non-selected data points (just 1.9% of all data points in our training data were selected), it's important to give the class of selected data points a higher weight when training. In fact, without establishing these class weights our model would quickly acheive 98% accuracy by naively leaving all data points unselected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03pNFL5zTINT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_weight = len(mms_data)/(2*np.bincount(mms_data['selected'].values)[0])\n",
        "true_weight = len(mms_data)/(2*np.bincount(mms_data['selected'].values)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckN-_FOuTlI4",
        "colab_type": "text"
      },
      "source": [
        "Our entire dataset is not contigous, and it contains time intervals with no observations. Therefore, we break it up into contigous chunks. We can do so by breaking up the data into the windows that the SITLs used to review the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuoPyoU5UV3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sitl_windows = mms.mission_events('sroi', mms_data.index[0].to_pydatetime(), mms_data.index[-1].to_pydatetime(), sc='mms1')\n",
        "windows = []\n",
        "for start, end in zip(sitl_windows['tstart'], sitl_windows['tend']):\n",
        "  window = mms_data[start:end]\n",
        "  if not window.empty and len(window[window['selected']==True])>1:\n",
        "    windows.append(window)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyCpZWhRfIOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "windows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btKBuEPkUrXW",
        "colab_type": "text"
      },
      "source": [
        "Finally, we break up our data into individual sequences that will be fed to our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY_SGxXFU88m",
        "colab_type": "text"
      },
      "source": [
        "We define a SEQ_LEN variable that will determine the length of our sequences. This variable will also be passed to our network so that it knows how long of a data sequence to expect while training. The choice of sequence length is largely arbitrary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQzSPJXkU7J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXhi2H2j7x6",
        "colab_type": "text"
      },
      "source": [
        "For each window, we assemble two sequences: an X_sequence containing individual data points from our training data and a y_sequence containing the truth values for those data points (whether or not those data points were selected by a SITL). \n",
        "\n",
        "We add those sequences to four collections: X_train and y_train containing X_sequences and y_sequences for our training data and X_test and y_test containing X_sequences and y_sequences for our testing data. We allocate 80% of the sequences to trainining and the remaining 20% to testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ajG41MVJWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " while True:\n",
        "    X_train, X_test, y_train, y_test = [], [], [], []\n",
        "\n",
        "    sequences = []\n",
        "    for i in range(len(windows)):\n",
        "      X_sequence = []\n",
        "      y_sequence = []\n",
        "\n",
        "      if random.random() < 0.6:\n",
        "        for value in windows[i].values:\n",
        "          X_sequence.append(value[:-1])\n",
        "          y_sequence.append(value[-1])\n",
        "          if len(X_sequence) == SEQ_LEN:\n",
        "            X_train.append(X_sequence.copy())\n",
        "            \n",
        "            y_train.append(y_sequence.copy())\n",
        "\n",
        "            X_sequence = []\n",
        "            y_sequence = []\n",
        "\n",
        "      else:\n",
        "        for value in windows[i].values:\n",
        "          X_sequence.append(value[:-1])\n",
        "          y_sequence.append(value[-1])\n",
        "          if len(X_sequence) == SEQ_LEN:\n",
        "            X_test.append(X_sequence.copy())\n",
        "            \n",
        "            y_test.append(y_sequence.copy())\n",
        "\n",
        "            X_sequence = []\n",
        "            y_sequence = []\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.expand_dims(np.array(y_train), axis=2)\n",
        "    y_test = np.expand_dims(np.array(y_test), axis=2)\n",
        "\n",
        "    if len(X_train) > len(X_test):\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcGTxO4tYIEa",
        "colab_type": "text"
      },
      "source": [
        "We can see how many sequences of data we have for training and testing, respectively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Xc30NSYIgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of sequences in training data: {len(X_train)}\")\n",
        "print(f\"Number of sequences in test data: {len(X_test)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRsSmLuDs029",
        "colab_type": "text"
      },
      "source": [
        "# Define and Train LSTM\n",
        "\n",
        "Now that we have processed our data into our training and test sets, we can begin to build and train and our LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Hqo34gs4cx",
        "colab_type": "text"
      },
      "source": [
        "First, we need to define a custom F1 score and weighted binary crossentropy functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPl9Nicf4K_d",
        "colab_type": "text"
      },
      "source": [
        "An F1 score is a measure of a model's accuracy, calculated as a balance of the model's precision (the number of true positives predicted by the model divided by the total number of positives predicted by the model) and recall (the number of true positives predicted by the model divided by the number of actual positives in the data):\n",
        "\n",
        "![F1 = 2 * (precision * recall) / (precision + recall)](https://wikimedia.org/api/rest_v1/media/math/render/svg/1bf179c30b00db201ce1895d88fe2915d58e6bfd)\n",
        "\n",
        "We will evaluate our model using the F1 score since we want to strike a balance between the model's precision and recall. Remember, we cannot use true accuracy (the number of true positives and true negatives divided by the number of data points in the data) because of the imbalance between our classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKhoUVlFmfUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Credit: Paddy and Kev1n91 from https://stackoverflow.com/a/45305384/3988976)\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVoV71O4MAD",
        "colab_type": "text"
      },
      "source": [
        "Cross-entropy is a function used to determine the loss between a set of predictions and their truth values. The larger the difference between a prediction and its true value, the larger the loss will be. In general, many machine learning architectures (including our LSTM) are designed to minimize their given loss function. A perfect model will have a loss of 0.\n",
        "\n",
        "Binary cross-entropy is used when we only have two classes (in our case, selected or not selected) and weighted binary cross-entropy allows us to assign a weight to one of the classes. This weight can effectively increase or decrease the loss of that class. In our case, we have previously defined a variable *true_weight* to be the class weight for positive (selected) datapoints. We will pass that weight into the function.\n",
        "\n",
        "This cross-entropy function will be passed in to our model as our loss function.\n",
        "\n",
        "(Because the loss function of a model needs to be differentiable to perform gradient descent, we cannot use our F1 score as our loss function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBxIZXbMpJG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Credit: tobigue from https://stackoverflow.com/questions/42158866/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu)\n",
        "def weighted_binary_crossentropy(target, output):\n",
        "    \"\"\"\n",
        "    Weighted binary crossentropy between an output tensor \n",
        "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
        "    for the positive targets.\n",
        "\n",
        "    Combination of the following functions:\n",
        "    * keras.losses.binary_crossentropy\n",
        "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
        "    * tf.nn.weighted_cross_entropy_with_logits\n",
        "    \"\"\"\n",
        "    # transform back to logits\n",
        "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
        "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
        "    output = tf.log(output / (1 - output))\n",
        "    # compute weighted loss\n",
        "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
        "                                                    logits=output,\n",
        "                                                    pos_weight=true_weight)\n",
        "    return tf.reduce_mean(loss, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ScqhL295sl",
        "colab_type": "text"
      },
      "source": [
        "Before building our LSTM, we define several hyperparameters that will define how the model is trained:\n",
        "\n",
        "EPOCHS: The number of times the model trains through our entire dataset\n",
        "\n",
        "BATCH_SIZE: The number of sequences that our model trains using at any given point\n",
        "\n",
        "LAYER_SIZE: The number of LSTM internal to each layer of the model.\n",
        "\n",
        "Choices for these hyperparameters are largely arbitrary and can be altered to tune our LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7HBAE5wwTDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 128\n",
        "LAYER_SIZE = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMHwIiE9Aec9",
        "colab_type": "text"
      },
      "source": [
        "We now define our LSTM.\n",
        "\n",
        "For this version of the model, we two bidirectional LSTM layers, two dropout layers, and one time distributed dense layer.\n",
        "\n",
        "Internally, an LSTM layer uses a for loop to iterate over the timesteps of a sequence, while maintaining states that encode information from those timesteps. Using these internal states, the LSTM learns the characteristics of our data (the X_sequences we defined earlier) and how those data relate to our expected output (the y_sequences we defined earlier). Normal (unidirectional) LSTMs only encode information from prior-seen timesteps. Bidirectional LSTMs can can encode information prior to and after a given timestep.\n",
        "\n",
        "With the addition of a dense layer, the LSTM will output a value between 0 and 1 that corresponds to the model's certainty about whether or not a timestep was selected by the SITL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jU7BfnJBj0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = f\"{SEQ_LEN}-SEQ_LEN-{BATCH_SIZE}-BATCH_SIZE-{LAYER_SIZE}-LAYER_SIZE-{int(time.time())}\"\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(LAYER_SIZE, return_sequences=True), input_shape=(None, X_train.shape[2])))\n",
        "\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Bidirectional(LSTM(LAYER_SIZE, return_sequences=True), input_shape=(None, X_train.shape[2])))\n",
        "\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(loss=weighted_binary_crossentropy,\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy', f1, tf.keras.metrics.Precision()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asBMdOujBrbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIaRlNZJBkNE",
        "colab_type": "text"
      },
      "source": [
        "We set our training process to save the best versions of our model according to the previously defined F1 score. Each epoch, if a version of the model is trained with a higher F1 score than the previous best, the model saved on disk will be overwritten with the current best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFaaHkcwCP7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"mp-dl-unh\" \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8TWoDyKCRrE",
        "colab_type": "text"
      },
      "source": [
        "The following will train the model and save the training history for later visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCHQs29JCZMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "  x=X_train, y=y_train,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_data=(X_test, y_test),\n",
        "  callbacks=[checkpoint],\n",
        "  verbose=1,\n",
        "  shuffle=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzBPJKF0Q5sW",
        "colab_type": "text"
      },
      "source": [
        "# Performance Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWCIq-enCh3U",
        "colab_type": "text"
      },
      "source": [
        "To evaluate the training of our model over time, we visualize the model's loss on its training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhneNTcYChdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model train vs testing loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'testing'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlTr3Cs4VNOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot.plot(history.history['f1'])\n",
        "pyplot.plot(history.history['val_f1'])\n",
        "pyplot.title('model train vs testing f1')\n",
        "pyplot.ylabel('f1')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'testing'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQnqV-Kaw6fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot.plot(history.history['precision'])\n",
        "pyplot.plot(history.history['val_precision'])\n",
        "pyplot.title('model train vs testing precision')\n",
        "pyplot.ylabel('precision')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'testing'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8cXg6gpsZbN",
        "colab_type": "text"
      },
      "source": [
        "(We can see that the model performs much better on its training data. This is expected, as the model learns to recreate the selections of the training data. We can also see that the performance of the model on the testing data decreases over time. This is evidence of the model overfitting. At some point, the model begins to naively recreate the selections of the training data rather than truly learning how to make selections. In practice, we effectively ignore this as we have already saved the version of the model with the best performance on the testing data - mitigating any overfitting.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVdN-Urh-mdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('/content/mp-dl-unh', {'weighted_binary_crossentropy':weighted_binary_crossentropy, 'f1':f1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K2Hd8FlY2Xty"
      },
      "source": [
        "# Model Performance Visualization\n",
        "\n",
        "Now that we have trained the model, we will visualize its selection-making ability compared to the SITLs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jwbEhnbi2Xt0"
      },
      "source": [
        "Since we've already preprocessed the testing/training data into a format suitable for model training, we reload that data to preprocess it into a format suitable for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loaf_Rpz2Xt0",
        "colab": {}
      },
      "source": [
        "validation_data = pd.read_csv('training_data.csv', index_col=0, infer_datetime_format=True,\n",
        "\t\t\t\t\t\t   parse_dates=[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oujploNp2Xt4"
      },
      "source": [
        "We apply the same preprocessing steps to this data as we did for the original training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UarjtN552Xt4",
        "colab": {}
      },
      "source": [
        "index = validation_data.index\n",
        "selections = validation_data.pop(\"selected\")\n",
        "column_names = validation_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PB666aem2Xt7",
        "colab": {}
      },
      "source": [
        "validation_data = validation_data.replace([np.inf, -np.inf], np.nan)\n",
        "validation_data = validation_data.interpolate(method='time', limit_area='inside')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DtkJHVze2Xt-",
        "colab": {}
      },
      "source": [
        "validation_data = scaler.transform(validation_data)\n",
        "validation_data = pd.DataFrame(validation_data, index, column_names)\n",
        "validation_data = validation_data.join(selections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k84Veyug2XuC",
        "colab": {}
      },
      "source": [
        "validation_X = validation_data.values[:,:-1]\n",
        "validation_y = validation_data.values[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zux-H0AMLRF0",
        "colab_type": "text"
      },
      "source": [
        "Using the model we trainend earlier, we make test predctions on our validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QWHQ9x8x2XuH",
        "colab": {}
      },
      "source": [
        "test_predictions = model.predict(np.expand_dims(validation_X, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTUqi-6C2XuJ"
      },
      "source": [
        "We visualize the true SITL selections made over the validation data by plotting the ground truth values for each datapoint in the data (where a 1 denotes that an individual datapoint was selected and a 0 denotes that it wasn't).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izyEw0C32XuJ",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(validation_y.astype(int))\n",
        "plt.show\n",
        "pyplot.ylabel('Selected (1) or not (0)')\n",
        "pyplot.xlabel('Datapoints')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C_f_rD7T2XuM"
      },
      "source": [
        "...and we do the same for the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRFwAeCF2XuN",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(test_predictions.squeeze())\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzomETjp3tUa",
        "colab_type": "text"
      },
      "source": [
        "From this plot, we can see the continuous nature of the model's predictions. As mentioned earlier, the model outputs a continuous value between 0 and 1 for each datapoint that(very roughly) corresponds to its confidence in the selection of a point (i.e. an outputted value of 0.95 for a datapoint roughly means that the model is 95% certain that that point should be selected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KRuZkV0t2XuP"
      },
      "source": [
        "With this in mind, we filter the model's predictions so that only those predictions with a >= 50% probability of being a magnetopause crossing are kept. This choice of probability/certainty is known as the threshold. \n",
        "\n",
        "This choice of threshold is chosen to optimize between over-selecting datapoints (resulting in more false-positives) and under-selecting them (resulting in more false-negatives).\n",
        "\n",
        "As an example, consider an email server's spam-detection system. Such a system might have a fairly high threshold (>99%), as you don't want to accidentally send a user's non-spam email to their spam inbox. At the same time, it's okay if a handful of spam emails make it through their regular inbox.\n",
        "\n",
        "In our case, we can afford to over-select datapoints as we do not want to miss out on any potential magnetopause crossings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qq6_n8gP2XuP",
        "colab": {}
      },
      "source": [
        "t_output = [0 if x < 0.5 else 1 for x in test_predictions.squeeze()]\n",
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(t_output)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--HNA2LF-AE4",
        "colab_type": "text"
      },
      "source": [
        "# Model Validation\n",
        "\n",
        "Although we have already validated our model on data it has not seen (the testing set), we need to make sure that its ability to select magnetopause crossings is transferable to another range of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfv571-4MdaP",
        "colab_type": "text"
      },
      "source": [
        "We load a third set of data, the validation set, which serves as an independent check on the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YUj-vy80I2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = pd.read_csv('validation_data.csv', index_col=0, infer_datetime_format=True,\n",
        "\t\t\t\t\t\t   parse_dates=[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsg-zH1Y0I1p",
        "colab_type": "text"
      },
      "source": [
        "We apply the same preprocessing steps to the validation data as we did for the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3AsLjh-xsYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = validation_data.index\n",
        "selections = validation_data.pop(\"selected\")\n",
        "column_names = validation_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMXckJVX0QMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = validation_data.replace([np.inf, -np.inf], np.nan)\n",
        "validation_data = validation_data.interpolate(method='time', limit_area='inside')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcgfN3Y20bs8",
        "colab_type": "text"
      },
      "source": [
        "However, we standardize the validation data to the scale of the training/testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2pTbLeH0Ryk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = scaler.transform(validation_data)\n",
        "validation_data = pd.DataFrame(validation_data, index, column_names)\n",
        "validation_data = validation_data.join(selections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmvIfkOQ0S1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_X = validation_data.values[:,:-1]\n",
        "validation_y = validation_data.values[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iuoi7WQ53gk",
        "colab_type": "text"
      },
      "source": [
        "Using the model we trained earlier, we make test predctions on our validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlOFmA_R0Wm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = model.predict(np.expand_dims(validation_X, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZZPb5EjM9EB",
        "colab_type": "text"
      },
      "source": [
        "We visualize the true SITL selections made over the validation data in the same way we did above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42AfNry2FO7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(validation_y.astype(int))\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSdniM9HND_c",
        "colab_type": "text"
      },
      "source": [
        "...and we do the same for the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQvn147CFVgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(test_predictions.squeeze())\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbr_DnYhNJOJ",
        "colab_type": "text"
      },
      "source": [
        "Once again, we filter the model's predictions so that only those predictions with a >= 50% probability of being a magnetopause crossing are kept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKbbKKOAFZ7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_output = [0 if x < 0.5 else 1 for x in test_predictions.squeeze()]\n",
        "plt.figure(figsize=(28, 5))\n",
        "plt.plot(t_output)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z42aU4b4ieu",
        "colab_type": "text"
      },
      "source": [
        "We now plot a receiver operating characteristic (ROC) curve based on the model's performance over the evaluation data. \n",
        "\n",
        "An ROC curve will plot a model's true-positive vs. false positive rates of predictions for varying choices of thresholds. As the threshold approaches 1, the false positive rate and the true positive rates approach 0, as every prediction made is over the threshold and is thus considered a selection. As the threshold approaches 1, the false positive rate and the true positive rates approach 0, as no prediction made surpasses the threshold of 1.\n",
        "\n",
        "While we can use the plot to determine where we want to set our threshold (considering the importance of under-selecting or over-selecting points), it is more often used to get a sense of the performance of our model.\n",
        "\n",
        "To do so, we calculate the total area under the ROC curve. This area is equal to the probability that the model will output a higher prediction value for a randomly chosen datapoint whose ground truth was \"selected\" than for a randomly chosen datapoint whose ground truth value was \"not selected\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62howqH4NP2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(validation_y.astype(int), test_predictions.squeeze())\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve - AUC = {:.2f}'.format(auc(fpr, tpr)))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2X1ovuuNyE6",
        "colab_type": "text"
      },
      "source": [
        "Finally, we generate a list of predicted selection windows. The following code groups contiguous selected datapoints into windows and list the start and dates of those windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC-65ktvIKUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicts_df = pd.DataFrame()\n",
        "predicts_df.insert(0, \"time\", validation_data.index)\n",
        "predicts_df.insert(1, \"prediction\", t_output)\n",
        "predicts_df['group'] = (predicts_df.prediction != predicts_df.prediction.shift()).cumsum()\n",
        "predicts_df = predicts_df.loc[predicts_df['prediction'] == True]\n",
        "selections = pd.DataFrame({'BeginDate' : predicts_df.groupby('group').time.first(), \n",
        "              'EndDate' : predicts_df.groupby('group').time.last()})\n",
        "selections = selections.set_index('BeginDate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db_RET1gfmLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selections"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}